{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqVW5Nezc8GZ"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check Python version, PyTorch, GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGkS6nN6dQaz",
        "outputId": "f49e39b2-eb9a-4d27-82bf-b19a58367a7e"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngcaTx9Y-1lX",
        "outputId": "53a7f8bf-cb40-4d98-da60-3cbfe097fc31"
      },
      "outputs": [],
      "source": [
        "!pip list | grep torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FSlIfLxdFB6",
        "outputId": "ded9414e-14e8-4194-ed38-69d491ab6f3a"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "# It's okay to run this notebook without a GPU!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CuuBRbRdXDJ"
      },
      "source": [
        "## Simple linear MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate and visualize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "ZhJh0fUDdIlo",
        "outputId": "6658426c-f154-4bf3-eb28-a21091b5b469"
      },
      "outputs": [],
      "source": [
        "# x is 1-dimensional\n",
        "\n",
        "n = 50\n",
        "\n",
        "d = 1\n",
        "x = np.random.uniform(-1, 1, (n, d))\n",
        "\n",
        "# y = 5x + 10\n",
        "weights_true = np.array([[5],])\n",
        "bias_true = np.array([10])\n",
        "\n",
        "y_true = x @ weights_true + bias_true\n",
        "print(f'x: {x.shape}, weights: {weights_true.shape}, bias: {bias_true.shape}, y: {y_true.shape}')\n",
        "\n",
        "plt.plot(x, y_true, marker='x', label='underlying function')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKASk4TeP-Vu"
      },
      "source": [
        "### Basic prediction function: Linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "rpaA6KL1AzUe",
        "outputId": "af85d4ed-1972-4d54-fd0e-758160896961"
      },
      "outputs": [],
      "source": [
        "# Let's initialize our predictions\n",
        "\n",
        "class Linear:\n",
        "  def __init__(self, input_dim, num_hidden=1):\n",
        "    # The initialization is important to properly deal with different\n",
        "    # input sizes (otherwise gradients quickly go to 0).\n",
        "    self.weights = np.random.randn(input_dim, num_hidden) * np.sqrt(2. / input_dim)\n",
        "    self.bias = np.zeros(num_hidden)\n",
        "  \n",
        "  def __call__(self, x):\n",
        "    return x @ self.weights + self.bias\n",
        "\n",
        "linear = Linear(d)\n",
        "y_pred = linear(x)\n",
        "plt.plot(x, y_true, marker='x', label='underlying function')\n",
        "plt.scatter(x, y_pred, color='r', marker='.', label='our function')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtKQdlViQBm_"
      },
      "source": [
        "### Basic loss function: MSE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIn86q0oDNsy",
        "outputId": "756200f6-7830-43c4-bd27-9119d23f3f60"
      },
      "outputs": [],
      "source": [
        "# How wrong are these initial predictions, exactly?\n",
        "# It's up to us, and our definition is called the loss function.\n",
        "# Let's use Mean Squared Error (MSE) as our loss function.\n",
        "\n",
        "class MSE:\n",
        "  def __call__(self, y_pred, y_true):\n",
        "    self.y_pred = y_pred\n",
        "    self.y_true = y_true\n",
        "    return ((y_true - y_pred) ** 2).mean()\n",
        "  \n",
        "loss = MSE()\n",
        "print(f'Our initial loss is {loss(y_pred, y_true)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMJrVabQRbBp"
      },
      "source": [
        "### Add back propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNw1AvS0ArB3"
      },
      "outputs": [],
      "source": [
        "# Let's use gradient descent to learn the weights and bias that minimizes the loss function.\n",
        "# For this, we need the gradient of the loss function and the gradients of the linear function.\n",
        "\n",
        "class MSE:\n",
        "  def __call__(self, y_pred, y_true):\n",
        "    self.y_pred = y_pred\n",
        "    self.y_true = y_true\n",
        "    return ((y_pred - y_true) ** 2).mean()\n",
        "\n",
        "  def backward(self):\n",
        "    n = self.y_true.shape[0]\n",
        "    self.gradient = 2. * (self.y_pred - self.y_true) / n\n",
        "    # print('MSE backward', self.y_pred.shape, self.y_true.shape, self.gradient.shape)\n",
        "    return self.gradient\n",
        "\n",
        "\n",
        "class Linear:\n",
        "  def __init__(self, input_dim: int, num_hidden: int = 1):\n",
        "    self.weights = np.random.randn(input_dim, num_hidden) * np.sqrt(2. / input_dim)\n",
        "    self.bias = np.zeros(num_hidden)\n",
        "  \n",
        "  def __call__(self, x):\n",
        "    self.x = x\n",
        "    output = x @ self.weights + self.bias\n",
        "    return output\n",
        "\n",
        "  def backward(self, gradient):\n",
        "    self.weights_gradient = self.x.T @ gradient\n",
        "    self.bias_gradient = gradient.sum(axis=0)\n",
        "    self.x_gradient = gradient @ self.weights.T\n",
        "    return self.x_gradient\n",
        "\n",
        "  def update(self, lr):\n",
        "    self.weights = self.weights - lr * self.weights_gradient\n",
        "    self.bias = self.bias - lr * self.bias_gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NcA_JDXJ6rR",
        "outputId": "4b109598-71d9-41f9-82f8-262c7e5d114e"
      },
      "outputs": [],
      "source": [
        "# Take one step forward and one step backward to make sure nothing breaks, and that the loss decreases.\n",
        "loss = MSE()\n",
        "linear = Linear(d)\n",
        "y_pred = linear(x)\n",
        "print(loss(y_pred, y_true))\n",
        "loss_gradient = loss.backward()\n",
        "linear.backward(loss_gradient)\n",
        "linear.update(0.1)\n",
        "y_pred = linear(x)\n",
        "print(loss(y_pred, y_true))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoyuH3gCRjGp"
      },
      "source": [
        "### Train using gradient descent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "qE__Ygl2c4IS",
        "outputId": "4f6ecd59-7b83-46a4-a595-aa729e08b5dc"
      },
      "outputs": [],
      "source": [
        "plt.plot(x, y_true, marker='x', label='underlying function')\n",
        "\n",
        "loss = MSE()\n",
        "linear = Linear(d)\n",
        "\n",
        "num_epochs = 40\n",
        "lr = 0.1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  y_pred = linear(x)\n",
        "  loss_value = loss(y_pred, y_true)\n",
        "\n",
        "  if epoch % 5 == 0:\n",
        "    print(f'Epoch {epoch}, loss {loss_value}')\n",
        "    plt.plot(x, y_pred.squeeze(), label=f'Epoch {epoch}')\n",
        "\n",
        "  gradient_from_loss = loss.backward()\n",
        "  linear.backward(gradient_from_loss)\n",
        "  linear.update(lr)\n",
        "\n",
        "plt.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK1qniplRpxN"
      },
      "source": [
        "### 2-dimensional inputs work, too"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "RWqEaOWqNbwV",
        "outputId": "05dc9030-9622-499b-c218-f23e45a23a8a"
      },
      "outputs": [],
      "source": [
        "# What about 2-dimensional x?\n",
        "\n",
        "n = 100\n",
        "d = 2\n",
        "x = np.random.uniform(-1, 1, (n, d))\n",
        "\n",
        "# y = w * x + b\n",
        "# y = w_0 * x_0 + w_1 * x_1 + b\n",
        "# y = w@x + b\n",
        "\n",
        "weights_true = np.array([[2, -1], ]).T\n",
        "bias_true = np.array([0.5])\n",
        "print(x.shape, weights_true.shape, bias_true.shape)\n",
        "\n",
        "y_true = x @ weights_true + bias_true\n",
        "print(f'x: {x.shape}, weights: {weights_true.shape}, bias: {bias_true.shape}, y: {y_true.shape}')\n",
        "\n",
        "def plot_3d(x, y, y_pred=None):\n",
        "  import matplotlib.pyplot as plt\n",
        "  from mpl_toolkits.mplot3d import Axes3D\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(111, projection='3d')\n",
        "  ax.scatter(x[:, 0], x[:, 1], y, label='underlying function')\n",
        "  if y_pred is not None:\n",
        "    ax.scatter(x[:, 0], x[:, 1], y_pred, label='our function')\n",
        "  plt.legend()\n",
        "\n",
        "plot_3d(x, y_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "pZwGG7rAUJfe",
        "outputId": "768e5e4a-c3ee-4958-dd8a-e98a18886c6c"
      },
      "outputs": [],
      "source": [
        "loss = MSE()\n",
        "linear = Linear(2)\n",
        "y_pred = linear(x)\n",
        "print(loss(y_pred, y_true))\n",
        "fig = plot_3d(x, y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "ZydPQYKIUtNT",
        "outputId": "368cae8a-9a6a-46c2-efdf-fc338ea3d6b6"
      },
      "outputs": [],
      "source": [
        "from typing import Callable\n",
        "\n",
        "def fit(x: np.ndarray, y: np.ndarray, model: Callable, loss: Callable, lr: float, num_epochs: int):\n",
        "  for epoch in range(num_epochs):\n",
        "    y_pred = model(x)\n",
        "    loss_value = loss(y_pred, y)\n",
        "    print(f'Epoch {epoch}, loss {loss_value}')\n",
        "    gradient_from_loss = loss.backward()\n",
        "    model.backward(gradient_from_loss)\n",
        "    model.update(lr)\n",
        "\n",
        "fit(x, y_true, model=linear, loss=loss, lr=0.1, num_epochs=20)\n",
        "plot_3d(x, y_true, linear(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVZo5o9hdcia"
      },
      "source": [
        "## Basic regression with a Multi-layer Perceptron\n",
        "\n",
        "So, we now have a way to automatically fit a linear function to N-dimensional data.\n",
        "\n",
        "How can this be made to work for non-linear data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "cqENhTMpdlJT",
        "outputId": "a2a27ec5-5154-4028-808b-fece1a29259a"
      },
      "outputs": [],
      "source": [
        "# Make non-linear data\n",
        "\n",
        "n = 200\n",
        "d = 2\n",
        "x = np.random.uniform(-1, 1, (n, d))\n",
        "\n",
        "weights_true = np.array([[5, 1],]).T\n",
        "bias_true = np.array([1])\n",
        "\n",
        "y_true = (x ** 2) @ weights_true + x @ weights_true + bias_true\n",
        "print(f'x: {x.shape}, weights: {weights_true.shape}, bias: {bias_true.shape}, y: {y_true.shape}')\n",
        "\n",
        "plot_3d(x, y_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 928
        },
        "id": "UgbM2NMJYswq",
        "outputId": "a32c91ce-f04b-455f-bfa0-a813ce409aff"
      },
      "outputs": [],
      "source": [
        "# We can train just fine, but the final loss will remain high, as our linear function is incapable\n",
        "# of representing the data.\n",
        "\n",
        "loss = MSE()\n",
        "linear = Linear(d)\n",
        "fit(x, y_true, model=linear, loss=loss, lr=0.1, num_epochs=40)\n",
        "plot_3d(x, y_true, linear(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-QK1DGMVFmV"
      },
      "source": [
        "### Add non-linearity: ReLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhGZIvzkYzar",
        "outputId": "6c87b0c3-81c3-4845-e8e6-5466f5026184"
      },
      "outputs": [],
      "source": [
        "# In order to learn non-linear functions, we need non-linearities in our model.\n",
        "\n",
        "class Relu:\n",
        "    def __call__(self, input_):\n",
        "        self.input_ = input_\n",
        "        self.output = np.clip(self.input_, 0, None)\n",
        "        return self.output\n",
        "    \n",
        "    def backward(self, output_gradient):\n",
        "      # import pdb; pdb.set_trace()  # By the way, this is how you can debug\n",
        "      self.input_gradient = (self.input_ > 0) * output_gradient\n",
        "      return self.input_gradient\n",
        "\n",
        "\n",
        "relu = Relu()\n",
        "input_ = np.expand_dims(np.array([1, 0.5, 0, -0.5, -1]), -1)\n",
        "print(relu(input_))\n",
        "print(relu.backward(input_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c67k16FsVQUj"
      },
      "source": [
        "### Train our new non-linear model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "hyKYKhm0ZunK",
        "outputId": "eb47c706-80b2-4ef7-989d-cf4b0d01d56e"
      },
      "outputs": [],
      "source": [
        "class Model:\n",
        "  def __init__(self, input_dim, num_hidden):\n",
        "    self.linear1 = Linear(input_dim, num_hidden)\n",
        "    self.relu = Relu()\n",
        "    self.linear2 = Linear(num_hidden, 1)\n",
        "  \n",
        "  def __call__(self, x):\n",
        "    l1 = self.linear1(x)\n",
        "    r = self.relu(l1)\n",
        "    l2 = self.linear2(r)\n",
        "    return l2\n",
        "  \n",
        "  def backward(self, output_gradient):\n",
        "    linear2_gradient = self.linear2.backward(output_gradient)\n",
        "    relu_gradient = self.relu.backward(linear2_gradient)\n",
        "    linear1_gradient = self.linear1.backward(relu_gradient)\n",
        "    # print('Model backward', linear2_gradient.shape, relu_gradient.shape, linear1_gradient.shape)\n",
        "    # import pdb; pdb.set_trace()\n",
        "    return linear1_gradient\n",
        "\n",
        "  def update(self, lr):\n",
        "    self.linear2.update(lr)\n",
        "    self.linear1.update(lr)\n",
        "\n",
        "loss = MSE()\n",
        "model = Model(d, 10)\n",
        "y_pred = model(x)\n",
        "loss_value = loss(y_pred, y_true)\n",
        "loss_gradient = loss.backward()\n",
        "print(loss_value)\n",
        "model.backward(loss_gradient)\n",
        "plot_3d(x, y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKJ4L0-WfYrd",
        "outputId": "8484df11-cdc2-40f7-8441-cef64ebcf4bd"
      },
      "outputs": [],
      "source": [
        "# Test just one forward and backward step\n",
        "loss = MSE()\n",
        "model = Model(d, 10)\n",
        "y_pred = model(x)\n",
        "loss_value = loss(y_pred, y_true)\n",
        "print(loss_value)\n",
        "loss_gradient = loss.backward()\n",
        "model.backward(loss_gradient)\n",
        "model.update(0.1)\n",
        "y_pred = model(x)\n",
        "loss_value = loss(y_pred, y_true)\n",
        "print(loss_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 928
        },
        "id": "wllnmL6Sbsh6",
        "outputId": "f634d39c-58cd-4e95-fa91-1cd139c6d81c"
      },
      "outputs": [],
      "source": [
        "fit(x, y_true, model=model, loss=loss, lr=0.1, num_epochs=40)\n",
        "plot_3d(x, y_true, model(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq0Z_14tdra9"
      },
      "source": [
        "### Same thing, in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUzGijLkOUiy",
        "outputId": "18e32079-c249-4dbf-bb02-8e59e7fe4521"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class TorchModel(nn.Module):\n",
        "  def __init__(self, input_dim, num_hidden):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(input_dim, num_hidden)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(num_hidden, 1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    l1 = self.linear1(x)\n",
        "    r = self.relu(l1)\n",
        "    l2 = self.linear2(r)\n",
        "    return l2\n",
        "\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "model = TorchModel(d, 10)\n",
        "x_tensor = torch.tensor(x).float()\n",
        "y_true_tensor = torch.tensor(y_true).float()\n",
        "y_pred_tensor = model(x_tensor)\n",
        "loss_value = loss(y_pred_tensor, y_true_tensor)\n",
        "print(loss_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDoKS0BBYlxU",
        "outputId": "84c60291-28a0-40c1-c6d8-716e6440d798"
      },
      "outputs": [],
      "source": [
        "# Test just one forward and backward step\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "y_pred_tensor = model(x_tensor)\n",
        "loss_value = loss(y_pred_tensor, y_true_tensor)\n",
        "print(loss_value)\n",
        "loss_gradient = loss_value.backward()\n",
        "optimizer.step()\n",
        "\n",
        "y_pred_tensor = model(x_tensor)\n",
        "loss_value = loss(y_pred_tensor, y_true_tensor)\n",
        "print(loss_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 928
        },
        "id": "Yx8hOdiRcIEJ",
        "outputId": "5682aabf-8e5a-4390-a636-b093fbf6b315"
      },
      "outputs": [],
      "source": [
        "# Now we run the training loop\n",
        "\n",
        "def torch_fit(x: np.ndarray, y: np.ndarray, model: Callable, loss: Callable, lr: float, num_epochs: int):\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "  for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    y_pred_tensor = model(x_tensor)\n",
        "    loss_value = loss(y_pred_tensor, y_true_tensor)\n",
        "    print(loss_value)\n",
        "    loss_value.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "torch_fit(x_tensor, y_true_tensor, model=model, loss=loss, lr=0.1, num_epochs=40)\n",
        "plot_3d(x, y_true, model(x_tensor).detach())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo5zDlScdl4s"
      },
      "source": [
        "### Same thing, in Tensorflow/Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "id": "HDoGEt6RnvCU",
        "outputId": "fea43c55-34f9-4c9c-ff0d-446c4853e280"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "inputs = keras.Input(shape=(2,))\n",
        "l1 = layers.Dense(10, activation='relu', name='dense_1')(inputs)\n",
        "outputs = layers.Dense(1, name='regression')(l1)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "print(model.summary())\n",
        "model.compile(loss='mse', optimizer=optimizers.SGD(0.1))\n",
        "\n",
        "model.fit(x, y_true, epochs=10)\n",
        "\n",
        "y_pred = model.predict(x)\n",
        "\n",
        "plot_3d(x, y_true, model(x))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "deep_learning_fundamentals_part1",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
